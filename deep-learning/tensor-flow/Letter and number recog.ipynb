{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip, binascii, struct, numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the classifier to recognize both MNIST numbers and NON-MNIST letters. Our training plan is approximately as follows:\n",
    "1) Load mnist data\n",
    "2) Load non-mnist data\n",
    "3) Merge both data sources into a single array\n",
    "4) Merge their label into a single array\n",
    "5) Split each training data set training and test.\n",
    "5.1) Merge training data sets for both mnist and non-mnist\n",
    "5.2) Merge test datasets\n",
    "5.3) Add \"empty\" training set\n",
    "5.4) Add \"empty\" test set\n",
    "6) Build Tensorf-flow model\n",
    "7) Train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directory = 'mnist'\n",
    "train_data_filename = directory + '/train-images-idx3-ubyte.gz'\n",
    "train_labels_filename = directory + '/train-labels-idx1-ubyte.gz'\n",
    "test_data_filename = directory + '/t10k-images-idx3-ubyte.gz'\n",
    "test_labels_filename = directory + '/t10k-labels-idx1-ubyte.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST dataset\n",
    " Extract the images into a 4D tensor [image index, y, x, channels]. For greyscale MNIST, the number of channels is always 1. Values are rescaled from [0, 255] down to [-0.5, 0.5]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_SIZE = 28\n",
    "PIXEL_DEPTH = 255\n",
    "\n",
    "def extract_data(filename, num_images):\n",
    "  print ('Extracting', filename)\n",
    "  with gzip.open(filename) as bytestream:\n",
    "    # Skip the magic number and dimensions; we know these values.\n",
    "    bytestream.read(16)\n",
    "    \n",
    "    buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images)\n",
    "    data = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.float32)\n",
    "    data = (data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
    "    data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, 1)\n",
    "    return data\n",
    "\n",
    "train_data_mnist = extract_data(train_data_filename, 60000)\n",
    "test_data_mnist = extract_data(test_data_filename, 10000)\n",
    "train_data_mnist.shape\n",
    "#plt.imshow(train_data[0].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, load MNIST labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "NUM_LABELS = 20\n",
    "\n",
    "def extract_labels(filename, num_images):\n",
    "  \"\"\"Extract the labels into a 1-hot matrix [image index, label index].\"\"\"\n",
    "  print ('Extracting', filename)\n",
    "  with gzip.open(filename) as bytestream:\n",
    "    # Skip the magic number and count; we know these values.\n",
    "    bytestream.read(8)\n",
    "    \n",
    "    buf = bytestream.read(1 * num_images)\n",
    "    labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n",
    "  # Convert to dense 1-hot representation.\n",
    "  return (numpy.arange(NUM_LABELS) == labels[:, None]).astype(numpy.float32)\n",
    "\n",
    "train_labels_mnist = extract_labels(train_labels_filename, 60000)\n",
    "test_labels_mnist = extract_labels(test_labels_filename, 10000)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the training data into 2 sets: training , validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: Train Valid Test (55000, 28, 28, 1) (5000, 28, 28, 1) (10000, 28, 28, 1)\n",
      "Labels: Train Valid Test (55000, 20) (5000, 20) (10000, 20)\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_SIZE = 5000\n",
    "\n",
    "valid_data_mnist = train_data_mnist[:VALIDATION_SIZE, :, :, :]\n",
    "valid_labels_mnist = train_labels_mnist[:VALIDATION_SIZE]\n",
    "train_data_mnist = train_data_mnist[VALIDATION_SIZE:, :, :, :]\n",
    "train_labels_mnist = train_labels_mnist[VALIDATION_SIZE:]\n",
    "\n",
    "print('Data:', 'Train', 'Valid', 'Test', train_data_mnist.shape, valid_data_mnist.shape, test_data_mnist.shape)\n",
    "print('Labels:', 'Train', 'Valid', 'Test', train_labels_mnist.shape, valid_labels_mnist.shape, test_labels_mnist.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NON-MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "pickle_file = directory + '/notMNIST.pickle'\n",
    "f = open(pickle_file, 'rb')\n",
    "data = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "nm_train_data = data['train_dataset']\n",
    "nm_train_labels = data['train_labels']\n",
    "\n",
    "nm_valid_data = data['valid_dataset']\n",
    "nm_valid_labels = data['valid_labels']\n",
    "\n",
    "nm_test_data = data['test_dataset']\n",
    "nm_test_labels = data['test_labels']\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 28, 28, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nm_test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape data array to match that of MNIST. Currently it's [N, h, w]. We want [N, h, w, channels], where in this case the channels == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nm_train_data = nm_train_data.reshape(-1, 28, 28, 1)\n",
    "nm_valid_data = nm_valid_data.reshape(-1, 28, 28, 1)\n",
    "nm_test_data = nm_test_data.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shift labels by 10. The first 10 indexes are for MNIST numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shift_by = 10\n",
    "nm_train_labels_real_adj = nm_train_labels + shift_by\n",
    "nm_valid_labels_adj = nm_valid_labels + shift_by\n",
    "nm_test_labels_adj = nm_test_labels + shift_by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert labels into one-hot encoding similar to MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_one_hot(real_labels):\n",
    "    return (numpy.arange(NUM_LABELS) == real_labels[:, None]).astype(np.float32)\n",
    "nm_train_labels = to_one_hot(nm_train_labels_real_adj)\n",
    "nm_valid_labels = to_one_hot(nm_valid_labels_adj)\n",
    "nm_test_labels = to_one_hot(nm_test_labels_adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge data arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(255000, 28, 28, 1) (17000, 28, 28, 1) (22000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "train_data_all = np.concatenate((train_data_mnist, nm_train_data))\n",
    "valid_data_all = np.concatenate((valid_data_mnist, nm_valid_data))\n",
    "test_data_all = np.concatenate((test_data_mnist, nm_test_data))\n",
    "print(train_data_all.shape, valid_data_all.shape, test_data_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge label arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(255000, 20) (17000, 20) (22000, 20)\n"
     ]
    }
   ],
   "source": [
    "train_labels_all = np.concatenate((train_labels_mnist, nm_train_labels))\n",
    "valid_labels_all = np.concatenate((valid_labels_mnist, nm_valid_labels))\n",
    "test_labels_all = np.concatenate((test_labels_mnist, nm_test_labels))\n",
    "print(train_labels_all.shape, valid_labels_all.shape, test_labels_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = train_labels_all.shape[0]\n",
    "train_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Build TensorFlow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "BATCH_SIZE = 120 #Each batch is the number of data we feed into NN each time\n",
    "# We have only one channel in our grayscale images.\n",
    "NUM_CHANNELS = 1\n",
    "# The random seed that defines initialization.\n",
    "SEED = 42\n",
    "\n",
    "# This is where training samples and labels are fed to the graph.\n",
    "# These placeholder nodes will be fed a batch of training data at each\n",
    "# training step, which we'll write once we define the graph structure.\n",
    "train_data_node = tf.placeholder(tf.float32, shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
    "train_labels_node = tf.placeholder(tf.float32,shape=(BATCH_SIZE, NUM_LABELS))\n",
    "\n",
    "# For the validation and test data, we'll just hold the entire dataset in one constant node.\n",
    "validation_data_node = tf.constant(valid_data_all)\n",
    "test_data_node = tf.constant(test_data_all)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The variables below hold all the trainable weights. For each, the\n",
    "# parameter defines how the variables will be initialized.\n",
    "conv1_weights = tf.Variable(tf.truncated_normal([5, 5, NUM_CHANNELS, 32],  # 5x5 filter, depth 32.\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1_biases = tf.Variable(tf.zeros([32]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv2_weights = tf.Variable(tf.truncated_normal([5, 5, 32, 64],stddev=0.1,seed=SEED))\n",
    "conv2_biases = tf.Variable(tf.constant(0.1, shape=[64]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fully connected, depth 512.\n",
    "fc1_weights = tf.Variable(tf.truncated_normal([int(IMAGE_SIZE / 4 * IMAGE_SIZE / 4 * 64), 512],stddev=0.1,seed=SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc1_biases = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "fc2_weights = tf.Variable(tf.truncated_normal([512, NUM_LABELS],stddev=0.1, seed=SEED))\n",
    "fc2_biases = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def model(data, train=False):\n",
    "  # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n",
    "  # the same size as the input). Note that {strides} is a 4D array whose\n",
    "  # shape matches the data layout: [image index, y, x, depth].\n",
    "  conv = tf.nn.conv2d(data,\n",
    "                      conv1_weights,\n",
    "                      strides=[1, 1, 1, 1],\n",
    "                      padding='SAME')\n",
    "\n",
    "  # Bias and rectified linear non-linearity.\n",
    "  relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "  \n",
    "  # Max pooling. The kernel size spec ksize also follows the layout of\n",
    "  # the data. Here we have a pooling window of 2, and a stride of 2.\n",
    "  pool = tf.nn.max_pool(relu,\n",
    "                        ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1],\n",
    "                        padding='SAME')\n",
    "  conv = tf.nn.conv2d(pool,\n",
    "                      conv2_weights,\n",
    "                      strides=[1, 1, 1, 1],\n",
    "                      padding='SAME')\n",
    "  relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "  pool = tf.nn.max_pool(relu,\n",
    "                        ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1],\n",
    "                        padding='SAME')\n",
    "  \n",
    "  # Reshape the feature map cuboid into a 2D matrix to feed it to the\n",
    "  # fully connected layers.\n",
    "  pool_shape = pool.get_shape().as_list()\n",
    "  reshape = tf.reshape(\n",
    "      pool,\n",
    "      [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "  \n",
    "  # Fully connected layer. Note that the '+' operation automatically\n",
    "  # broadcasts the biases.\n",
    "  hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "  \n",
    "  # Add a 50% dropout during training only. Dropout also scales\n",
    "  # activations such that no rescaling is needed at evaluation time.\n",
    "  if train:\n",
    "    hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "  return tf.matmul(hidden, fc2_weights) + fc2_biases\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Training computation: logits + cross-entropy loss.\n",
    "logits = model(train_data_node, True)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "  logits, train_labels_node))\n",
    "\n",
    "# L2 regularization for the fully connected parameters.\n",
    "regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "                tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
    "# Add the regularization term to the loss.\n",
    "loss += 5e-4 * regularizers\n",
    "\n",
    "# Optimizer: set up a variable that's incremented once per batch and\n",
    "# controls the learning rate decay.\n",
    "batch = tf.Variable(0)\n",
    "# Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "  0.01,                # Base learning rate.\n",
    "  batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "  train_size,          # Decay step.\n",
    "  0.95,                # Decay rate.\n",
    "  staircase=True)\n",
    "# Use simple momentum for the optimization.\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
    "                                       0.9).minimize(loss,\n",
    "                                                     global_step=batch)\n",
    "\n",
    "# Predictions for the minibatch, validation set and test set.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "# We'll compute them only once in a while by calling their {eval()} method.\n",
    "validation_prediction = tf.nn.softmax(model(validation_data_node))\n",
    "test_prediction = tf.nn.softmax(model(test_data_node))\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Create a new interactive session that we'll use in\n",
    "# subsequent code cells.\n",
    "s = tf.InteractiveSession()\n",
    "\n",
    "# Use our newly created session as the default for \n",
    "# subsequent operations.\n",
    "s.as_default()\n",
    "\n",
    "# Initialize all the variables we defined above.\n",
    "tf.initialize_all_variables().run()\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Grab the first BATCH_SIZE examples and labels.\n",
    "batch_data = train_data_all[:BATCH_SIZE, :, :, :]\n",
    "batch_labels = train_labels_all[:BATCH_SIZE]\n",
    "\n",
    "# This dictionary maps the batch data (as a numpy array) to the\n",
    "# node in the graph it should be fed to.\n",
    "feed_dict = {train_data_node: batch_data,\n",
    "             train_labels_node: batch_labels}\n",
    "\n",
    "# Run the graph and fetch some of the nodes.\n",
    "_, l, lr, predictions = s.run(\n",
    "  [optimizer, loss, learning_rate, train_prediction],\n",
    "  feed_dict=feed_dict)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error rate calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def error_rate(predictions, labels):\n",
    "  \"\"\"Return the error rate and confusions.\"\"\"\n",
    "  correct = numpy.sum(numpy.argmax(predictions, 1) == numpy.argmax(labels, 1))\n",
    "  total = predictions.shape[0]\n",
    "\n",
    "  error = 100.0 - (100 * float(correct) / float(total))\n",
    "\n",
    "  confusions = numpy.zeros([NUM_LABELS, NUM_LABELS], numpy.float32)\n",
    "  bundled = zip(numpy.argmax(predictions, 1), numpy.argmax(labels, 1))\n",
    "  for predicted, actual in bundled:\n",
    "    confusions[predicted, actual] += 1\n",
    "    \n",
    "  return error, confusions\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 of 2125\n",
      "Mini-batch loss: 8.08755 Error: 90.83333 Learning rate: 0.01000\n",
      "Validation error: 94.5\n",
      "Step 100 of 2125\n",
      "Mini-batch loss: 3.46176 Error: 9.16667 Learning rate: 0.01000\n",
      "Validation error: 72.2\n",
      "Step 200 of 2125\n",
      "Mini-batch loss: 3.26778 Error: 7.50000 Learning rate: 0.01000\n",
      "Validation error: 71.7\n",
      "Step 300 of 2125\n",
      "Mini-batch loss: 3.19688 Error: 4.16667 Learning rate: 0.01000\n",
      "Validation error: 71.4\n",
      "Step 400 of 2125\n",
      "Mini-batch loss: 3.11848 Error: 4.16667 Learning rate: 0.01000\n",
      "Validation error: 71.3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-eccae0479d09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m   _, l, lr, predictions = s.run(\n\u001b[1;32m     14\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;31m# Print out the loss periodically.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kalininalex/Documents/Dev/virtualenv/py3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 710\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    711\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kalininalex/Documents/Dev/virtualenv/py3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 908\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kalininalex/Documents/Dev/virtualenv/py3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 958\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    959\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/kalininalex/Documents/Dev/virtualenv/py3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    963\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kalininalex/Documents/Dev/virtualenv/py3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    945\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    946\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "steps = int(train_size / BATCH_SIZE)\n",
    "for step in range(steps):\n",
    "  # Compute the offset of the current minibatch in the data.\n",
    "  # Note that we could use better randomization across epochs.\n",
    "  offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "  batch_data = train_data_all[offset:(offset + BATCH_SIZE), :, :, :]\n",
    "  batch_labels = train_labels_all[offset:(offset + BATCH_SIZE)]\n",
    "  # This dictionary maps the batch data (as a numpy array) to the\n",
    "  # node in the graph it should be fed to.\n",
    "  feed_dict = {train_data_node: batch_data,\n",
    "               train_labels_node: batch_labels}\n",
    "  # Run the graph and fetch some of the nodes.\n",
    "  _, l, lr, predictions = s.run(\n",
    "    [optimizer, loss, learning_rate, train_prediction],\n",
    "    feed_dict=feed_dict)\n",
    "  \n",
    "  # Print out the loss periodically.\n",
    "  if step % 100 == 0:\n",
    "    error, _ = error_rate(predictions, batch_labels)\n",
    "    print('Step {:d} of {:d}'.format(step, steps))\n",
    "    print ('Mini-batch loss: {:.5f} Error: {:.5f} Learning rate: {:.5f}'.format(l, error, lr))\n",
    "    print ('Validation error: {:.1f}'.format(error_rate(validation_prediction.eval(), valid_labels_all)[0]))\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFfCAYAAACfj30KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11dc36908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def draw_sample(img):\n",
    "    img = img.reshape(28, 28)\n",
    "    plt.imshow(img, cmap=plt.cm.Greys)\n",
    "\n",
    "labels = '0123456789abcdefghij'\n",
    "#arr = nm_valid_data\n",
    "data_arr = valid_data_mnist\n",
    "label_arr = valid_labels_mnist\n",
    "\n",
    "index = 3454\n",
    "draw_sample(data_arr[index])\n",
    "print(labels[np.argmax(label_arr[index])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5]],\n",
       "\n",
       "       [[ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5]],\n",
       "\n",
       "       [[ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5]],\n",
       "\n",
       "       [[ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5]],\n",
       "\n",
       "       [[ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5]],\n",
       "\n",
       "       [[ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5]],\n",
       "\n",
       "       [[ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5]],\n",
       "\n",
       "       [[ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5]],\n",
       "\n",
       "       [[ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5]],\n",
       "\n",
       "       [[ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5]],\n",
       "\n",
       "       [[ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5]],\n",
       "\n",
       "       [[ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5]],\n",
       "\n",
       "       [[ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5]],\n",
       "\n",
       "       [[ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5]],\n",
       "\n",
       "       [[ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5]],\n",
       "\n",
       "       [[ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5]],\n",
       "\n",
       "       [[ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5]],\n",
       "\n",
       "       [[ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5]],\n",
       "\n",
       "       [[ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5]],\n",
       "\n",
       "       [[ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5]],\n",
       "\n",
       "       [[ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5]],\n",
       "\n",
       "       [[ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5]],\n",
       "\n",
       "       [[ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5]],\n",
       "\n",
       "       [[ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5]],\n",
       "\n",
       "       [[ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5]],\n",
       "\n",
       "       [[ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5]],\n",
       "\n",
       "       [[ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5]],\n",
       "\n",
       "       [[ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5],\n",
       "        [ 0.5]]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_all[214500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

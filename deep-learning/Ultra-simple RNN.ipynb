{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#-----------------------------------------------------------------------------------------\n",
    "class RNN:\n",
    "    def __init__(self):\n",
    "        self.text = 'hello'\n",
    "        self.vocabulary = sorted(set(list(self.text)))\n",
    "        self.vocab_size = len(self.vocabulary)\n",
    "        \n",
    "        self.W_x_to_hidden = None\n",
    "        self.W_hidden_to_hidden = None\n",
    "        self.W_hidden_to_y = None\n",
    "    \n",
    "        self.bias_hidden = None\n",
    "        self.bias_output = None\n",
    "    \n",
    "        self.h = [[0.]]\n",
    "    #-----------------------------------------------------------------------------------------\n",
    "    def clone(self):\n",
    "        ret = RNN()\n",
    "        ret.vocabulary = self.vocabulary\n",
    "        ret.vocab_size = self.vocab_size\n",
    "        \n",
    "        ret.W_x_to_hidden = self.W_x_to_hidden\n",
    "        ret.W_hidden_to_hidden = self.W_hidden_to_hidden\n",
    "        ret.W_hidden_to_y = self.W_hidden_to_y\n",
    "    \n",
    "        ret.bias_hidden = self.bias_hidden\n",
    "        ret.bias_output = self.bias_output\n",
    "    \n",
    "        self.h = [[0.]]\n",
    "        return ret\n",
    "    #-----------------------------------------------------------------------------------------\n",
    "    def reset(self):\n",
    "        self.h = [[0.]]\n",
    "        return self.h[0][0]\n",
    "    #-----------------------------------------------------------------------------------------\n",
    "    def load_weights(self):\n",
    "        self.W_x_to_hidden = np.array([[ 3.6, -4.8,  0.35, -0.26]])\n",
    "        self.W_hidden_to_hidden = np.array([[ 4.1]])\n",
    "        self.W_hidden_to_y = np.array([[-12.], [ -0.67], [ -0.85], [ 14.]])\n",
    "        self.bias_hidden = np.array([[ 0.41]])\n",
    "        self.bias_output = np.array([[-0.2], [-2.9], [ 6.1], [-3.4]])\n",
    "        \n",
    "        self.h = [[0.]]\n",
    "    #-----------------------------------------------------------------------------------------\n",
    "    def step(self, x_char):\n",
    "        assert isinstance(x_char, str)\n",
    "        x_index = self.vocabulary.index(x_char)\n",
    "        x_one_hot = np.zeros((self.vocab_size, 1))\n",
    "        x_one_hot[x_index] = 1\n",
    "\n",
    "        hidden_state = np.tanh(\\\n",
    "              np.dot(self.W_x_to_hidden, x_one_hot) \\\n",
    "            + np.dot(self.W_hidden_to_hidden, self.h)\\\n",
    "            + self.bias_hidden)\n",
    "        \n",
    "        y_output = np.dot(self.W_hidden_to_y, hidden_state) + self.bias_output\n",
    "\n",
    "        y_prob_after_softmax = np.exp(y_output) / np.sum(np.exp(y_output))\n",
    "        index = np.argmax(y_prob_after_softmax)\n",
    "        y_char = self.vocabulary[index]\n",
    "        \n",
    "        self.h = hidden_state # important to not forget to update the hidden state\n",
    "        \n",
    "        return y_char, self.h[0][0]\n",
    "    #-----------------------------------------------------------------------------------------\n",
    "    def loss(self, x_train, y_train, h_prev):\n",
    "        xs, hs, ps = {}, {}, {}\n",
    "        hs[-1] = h_prev.copy()\n",
    "        loss_value = 0.\n",
    "\n",
    "        # Forward propagation\n",
    "        for t, x_char in enumerate(x_train):\n",
    "            x_index = self.vocabulary.index(x_char)\n",
    "            x_one_hot = np.zeros((self.vocab_size, 1))\n",
    "            x_one_hot[x_index] = 1\n",
    "        \n",
    "            y_prob_target = np.zeros((self.vocab_size, 1))\n",
    "            y_index = self.vocabulary.index(y_train[t])\n",
    "            y_prob_target[y_index] = 1\n",
    "        \n",
    "            hidden_state = np.tanh(\\\n",
    "                  np.dot(self.W_x_to_hidden, x_one_hot) \\\n",
    "                + np.dot(self.W_hidden_to_hidden, hs[t - 1])\\\n",
    "                + self.bias_hidden)\n",
    "        \n",
    "            y_output = np.dot(self.W_hidden_to_y, hidden_state) + self.bias_output\n",
    "        \n",
    "            y_prob_after_softmax = np.exp(y_output) / np.sum(np.exp(y_output))\n",
    "\n",
    "            xs[t] = x_one_hot\n",
    "            hs[t] = hidden_state\n",
    "            ps[t] = y_prob_after_softmax\n",
    "\n",
    "            loss_value += -np.log(y_prob_after_softmax[y_index, 0])\n",
    "        \n",
    "        dWhy = np.zeros_like(self.W_hidden_to_y)\n",
    "        dWxh = np.zeros_like(self.W_x_to_hidden)\n",
    "        dWhh = np.zeros_like(self.W_hidden_to_hidden)\n",
    "        \n",
    "        dbh = np.zeros_like(self.bias_hidden)\n",
    "        dby = np.zeros_like(self.bias_output)\n",
    "        dh_next = np.zeros_like(hidden_state)\n",
    "\n",
    "        for t, x_char in reversed(list(enumerate(x_train))):\n",
    "            x_index = self.vocabulary.index(x_char)\n",
    "            x_one_hot = np.zeros((self.vocab_size, 1))\n",
    "            x_one_hot[x_index] = 1\n",
    "            \n",
    "            y_prob_target = np.zeros((self.vocab_size, 1))\n",
    "            y_index = self.vocabulary.index(y_train[t])\n",
    "            y_prob_target[y_index] = 1\n",
    "        \n",
    "            y_prob_after_softmax = ps[t]\n",
    "            hidden_state = hs[t]\n",
    "\n",
    "            dy = np.copy(y_prob_after_softmax)\n",
    "            dy -= y_prob_target\n",
    "        \n",
    "            dWhy += np.dot(dy, hidden_state.T)\n",
    "            dby += dy\n",
    "        \n",
    "            dh = np.dot(self.W_hidden_to_y.T, dy) + dh_next\n",
    "        \n",
    "            # tanh derivative and backprop\n",
    "            dh_raw = (1 - hidden_state ** 2) * dh\n",
    "        \n",
    "            dbh += dh_raw\n",
    "            dWxh += np.dot(dh_raw, x_one_hot.T)\n",
    "            dWhh += np.dot(dh_raw, hs[t - 1].T)\n",
    "        \n",
    "            dh_next = np.dot(self.W_hidden_to_hidden.T, dh_raw)\n",
    "        \n",
    "        for dp in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dp, -5, 5, out=dp) # mitigate exploding gradients\n",
    "        \n",
    "        return loss_value, dWxh, dWhh, dWhy, dbh, dby, hidden_state\n",
    "    #-----------------------------------------------------------------------------------------\n",
    "    def print_weights(self):\n",
    "        print(self.W_x_to_hidden)\n",
    "        print(self.W_hidden_to_hidden)\n",
    "        print(self.W_hidden_to_y)\n",
    "\n",
    "        print(self.bias_hidden)\n",
    "        print(self.bias_output)\n",
    "    #-----------------------------------------------------------------------------------------\n",
    "    def train(self, show=False):\n",
    "        self.W_x_to_hidden = np.random.randn(1, self.vocab_size)\n",
    "        self.W_hidden_to_hidden = np.array([[0.]])\n",
    "        self.W_hidden_to_y = np.random.randn(self.vocab_size, 1)\n",
    "    \n",
    "        self.bias_hidden = np.random.randn(1, 1)\n",
    "        self.bias_output = np.random.randn(self.vocab_size, 1)\n",
    "\n",
    "        text = list(self.text)\n",
    "        inputs = text[:len(text) - 1]\n",
    "        targets = text[1:]\n",
    "        # smooth_loss = -np.log(1.0 / self.vocab_size)\n",
    "        \n",
    "        learning_rate = 0.01\n",
    "        prev_loss = None\n",
    "        \n",
    "        for n in range(10000000):\n",
    "            h_prev = np.zeros((1, 1))\n",
    "            loss, dWxh, dWhh, dWhy, dbh, dby, hprev = self.loss(inputs, targets, h_prev)\n",
    "            # smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "            \n",
    "            if n % 10000 == 0: \n",
    "                print('iter {}, loss: {}, prev_loss: {}'.format(n, loss, prev_loss))\n",
    "                if prev_loss is not None and np.abs(prev_loss - loss) < 0.01:\n",
    "                    break\n",
    "                prev_loss = loss\n",
    "            \n",
    "            for param, dparam in zip([\n",
    "                    self.W_x_to_hidden, \n",
    "                    self.W_hidden_to_hidden, \n",
    "                    self.W_hidden_to_y, \n",
    "                    self.bias_hidden, \n",
    "                    self.bias_output],\n",
    "                                          [dWxh, dWhh, dWhy, dbh, dby]):\n",
    "                param += -learning_rate * dparam \n",
    "\n",
    "        print('loss: {}'.format(loss))\n",
    "        \n",
    "        if show:\n",
    "            self.print_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 6.15267754818036, prev_loss: None\n",
      "iter 10000, loss: 0.15620661114649903, prev_loss: 6.15267754818036\n",
      "iter 20000, loss: 0.06610570506554893, prev_loss: 0.15620661114649903\n",
      "iter 30000, loss: 0.04055294504242273, prev_loss: 0.06610570506554893\n",
      "iter 40000, loss: 0.3456545078401454, prev_loss: 0.04055294504242273\n",
      "iter 50000, loss: 0.26247759258899683, prev_loss: 0.3456545078401454\n",
      "iter 60000, loss: 0.016521952254129908, prev_loss: 0.26247759258899683\n",
      "iter 70000, loss: 0.012272806561888484, prev_loss: 0.016521952254129908\n",
      "loss: 0.012272806561888484\n",
      "('h', [[0.0]])\n",
      "('e', -0.99978868376414132)\n",
      "('l', -0.056773616904606111)\n",
      "('l', 0.27364530407626697)\n",
      "('o', 0.94466414990500724)\n"
     ]
    }
   ],
   "source": [
    "n = RNN()\n",
    "n.train()\n",
    "\n",
    "\n",
    "n.reset()\n",
    "print((n.text[0], n.h))\n",
    "\n",
    "\n",
    "for c in list(n.text[:-1]):\n",
    "    next = n.step(c)\n",
    "    print(next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "er = RNN()\n",
    "er.load_weights()\n",
    "# er.print_weights()\n",
    "\n",
    "er.reset()\n",
    "print(('i', er.h))\n",
    "next = er.step('i'); print(next)\n",
    "next = er.step(next[0]); print(next)\n",
    "next = er.step(next[0]); print(next)\n",
    "next = er.step(next[0]); print(next)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculations for the presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = RNN()\n",
    "n.load_weights()\n",
    "n.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input 'h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.99969249]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[0], \n",
    "              [1], \n",
    "              [0], \n",
    "              [0]])\n",
    "\n",
    "h_input = np.dot(n.W_hidden_to_hidden, n.h) + np.dot(n.W_x_to_hidden, x) + n.bias_hidden\n",
    "n.h = np.tanh(h_input)\n",
    "n.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 11.79630989],\n",
       "       [ -2.23020603],\n",
       "       [  6.94973862],\n",
       "       [-17.39569488]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.dot(n.W_hidden_to_y, n.h) + n.bias_output\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.92205162e-01],\n",
       "       [  8.03457698e-07],\n",
       "       [  7.79403445e-03],\n",
       "       [  2.08293106e-13]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.exp(y) / np.sum(np.exp(y))\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input 'e'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08850701]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1], \n",
    "              [0], \n",
    "              [0], \n",
    "              [0]])\n",
    "\n",
    "h_input = np.dot(n.W_hidden_to_hidden, n.h) + np.dot(n.W_x_to_hidden, x) + n.bias_hidden\n",
    "n.h = np.tanh(h_input)\n",
    "n.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.86208418],\n",
       "       [-2.8407003 ],\n",
       "       [ 6.17523096],\n",
       "       [-4.63909821]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.dot(n.W_hidden_to_y, n.h) + n.bias_output\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.90155904e-03],\n",
       "       [  1.20846860e-04],\n",
       "       [  9.94957586e-01],\n",
       "       [  2.00078804e-05]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.exp(y) / np.sum(np.exp(y))\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input 'l'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.37748309]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[0], \n",
    "              [0], \n",
    "              [1], \n",
    "              [0]])\n",
    "\n",
    "h_input = np.dot(n.W_hidden_to_hidden, n.h) + np.dot(n.W_x_to_hidden, x) + n.bias_hidden\n",
    "n.h = np.tanh(h_input)\n",
    "n.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.72979711],\n",
       "       [-3.15291367],\n",
       "       [ 5.77913937],\n",
       "       [ 1.88476329]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.dot(n.W_hidden_to_y, n.h) + n.bias_output\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.67428257e-05],\n",
       "       [  1.29431227e-04],\n",
       "       [  9.79896974e-01],\n",
       "       [  1.99468524e-02]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.exp(y) / np.sum(np.exp(y))\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input 'l'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.98039683]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[0], \n",
    "              [0], \n",
    "              [1], \n",
    "              [0]])\n",
    "\n",
    "h_input = np.dot(n.W_hidden_to_hidden, n.h) + np.dot(n.W_x_to_hidden, x) + n.bias_hidden\n",
    "n.h = np.tanh(h_input)\n",
    "n.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-11.96476199],\n",
       "       [ -3.55686588],\n",
       "       [  5.26666269],\n",
       "       [ 10.32555565]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.dot(n.W_hidden_to_y, n.h) + n.bias_output\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.07342092e-10],\n",
       "       [  9.29373665e-07],\n",
       "       [  6.31248169e-03],\n",
       "       [  9.93686589e-01]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.exp(y) / np.sum(np.exp(y))\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', 'd', 'e', 'h', 'l', 'o', 'r', 'w']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(set(list('hello world')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(list('it was the best')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(list('hello world')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#-----------------------------------------------------------------------------------------\n",
    "class RNN:\n",
    "    def __init__(self, text='hello'):\n",
    "        self.text = text\n",
    "        self.vocabulary = sorted(set(list(self.text)))\n",
    "        self.vocab_size = len(self.vocabulary)\n",
    "        \n",
    "        self.W_x_to_hidden = None\n",
    "        self.W_hidden_to_hidden = None\n",
    "        self.W_hidden_to_y = None\n",
    "    \n",
    "        self.bias_hidden = None\n",
    "        self.bias_output = None\n",
    "    \n",
    "        self.reset()\n",
    "    #-----------------------------------------------------------------------------------------\n",
    "    def reset(self):\n",
    "        self.h = np.array([[0.]])\n",
    "        return self.h[0][0]\n",
    "    #-----------------------------------------------------------------------------------------\n",
    "    def load_weights(self):\n",
    "        self.W_x_to_hidden = np.array([[ 3.6, -4.8,  0.35, -0.26]])\n",
    "        self.W_hidden_to_hidden = np.array([[ 4.1]])\n",
    "        self.W_hidden_to_y = np.array([[-12.], [ -0.67], [ -0.85], [ 14.]])\n",
    "        self.bias_hidden = np.array([[ 0.41]])\n",
    "        self.bias_output = np.array([[-0.2], [-2.9], [ 6.1], [-3.4]])\n",
    "        \n",
    "        self.reset()\n",
    "    #-----------------------------------------------------------------------------------------\n",
    "    def step(self, x_char):\n",
    "        assert isinstance(x_char, str)\n",
    "        x_index = self.vocabulary.index(x_char)\n",
    "        x_one_hot = np.zeros((self.vocab_size, 1))\n",
    "        x_one_hot[x_index] = 1\n",
    "\n",
    "        hidden_state = np.tanh(\\\n",
    "              np.dot(self.W_x_to_hidden, x_one_hot) \\\n",
    "            + np.dot(self.W_hidden_to_hidden, self.h)\\\n",
    "            + self.bias_hidden)\n",
    "        \n",
    "        y_output = np.dot(self.W_hidden_to_y, hidden_state) + self.bias_output\n",
    "\n",
    "        y_prob_after_softmax = np.exp(y_output) / np.sum(np.exp(y_output))\n",
    "        index = np.argmax(y_prob_after_softmax)\n",
    "        y_char = self.vocabulary[index]\n",
    "        \n",
    "        self.h = hidden_state # important to not forget to update the hidden state\n",
    "        \n",
    "        return y_char, self.h[0][0]\n",
    "    #-----------------------------------------------------------------------------------------\n",
    "    def loss(self, x_train, y_train, h_prev):\n",
    "        xs, hs, ps = {}, {}, {}\n",
    "        hs[-1] = h_prev.copy()\n",
    "        loss_value = 0.\n",
    "\n",
    "        # Forward propagation\n",
    "        for t, x_char in enumerate(x_train):\n",
    "            x_index = self.vocabulary.index(x_char)\n",
    "            x_one_hot = np.zeros((self.vocab_size, 1))\n",
    "            x_one_hot[x_index] = 1\n",
    "        \n",
    "            y_prob_target = np.zeros((self.vocab_size, 1))\n",
    "            y_index = self.vocabulary.index(y_train[t])\n",
    "            y_prob_target[y_index] = 1\n",
    "        \n",
    "            hidden_state = np.tanh(\\\n",
    "                  np.dot(self.W_x_to_hidden, x_one_hot) \\\n",
    "                + np.dot(self.W_hidden_to_hidden, hs[t - 1])\\\n",
    "                + self.bias_hidden)\n",
    "        \n",
    "            y_output = np.dot(self.W_hidden_to_y, hidden_state) + self.bias_output\n",
    "        \n",
    "            y_prob_after_softmax = np.exp(y_output) / np.sum(np.exp(y_output))\n",
    "\n",
    "            xs[t] = x_one_hot\n",
    "            hs[t] = hidden_state\n",
    "            ps[t] = y_prob_after_softmax\n",
    "\n",
    "            loss_value += -np.log(y_prob_after_softmax[y_index, 0])\n",
    "        \n",
    "        dWhy = np.zeros_like(self.W_hidden_to_y)\n",
    "        dWxh = np.zeros_like(self.W_x_to_hidden)\n",
    "        dWhh = np.zeros_like(self.W_hidden_to_hidden)\n",
    "        \n",
    "        dbh = np.zeros_like(self.bias_hidden)\n",
    "        dby = np.zeros_like(self.bias_output)\n",
    "        dh_next = np.zeros_like(hidden_state)\n",
    "\n",
    "        for t, x_char in reversed(list(enumerate(x_train))):\n",
    "            x_index = self.vocabulary.index(x_char)\n",
    "            x_one_hot = np.zeros((self.vocab_size, 1))\n",
    "            x_one_hot[x_index] = 1\n",
    "            \n",
    "            y_prob_target = np.zeros((self.vocab_size, 1))\n",
    "            y_index = self.vocabulary.index(y_train[t])\n",
    "            y_prob_target[y_index] = 1\n",
    "        \n",
    "            y_prob_after_softmax = ps[t]\n",
    "            hidden_state = hs[t]\n",
    "\n",
    "            dy = np.copy(y_prob_after_softmax)\n",
    "            dy -= y_prob_target\n",
    "        \n",
    "            dWhy += np.dot(dy, hidden_state.T)\n",
    "            dby += dy\n",
    "        \n",
    "            dh = np.dot(self.W_hidden_to_y.T, dy) + dh_next\n",
    "        \n",
    "            # tanh derivative and backprop\n",
    "            dh_raw = (1 - hidden_state ** 2) * dh\n",
    "        \n",
    "            dbh += dh_raw\n",
    "            dWxh += np.dot(dh_raw, x_one_hot.T)\n",
    "            dWhh += np.dot(dh_raw, hs[t - 1].T)\n",
    "        \n",
    "            dh_next = np.dot(self.W_hidden_to_hidden.T, dh_raw)\n",
    "        \n",
    "        for dp in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dp, -5, 5, out=dp) # mitigate exploding gradients\n",
    "        \n",
    "        return loss_value, dWxh, dWhh, dWhy, dbh, dby, hidden_state\n",
    "    #-----------------------------------------------------------------------------------------\n",
    "    def print_weights(self):\n",
    "        print(self.W_x_to_hidden)\n",
    "        print(self.W_hidden_to_hidden)\n",
    "        print(self.W_hidden_to_y)\n",
    "\n",
    "        print(self.bias_hidden)\n",
    "        print(self.bias_output)\n",
    "    #-----------------------------------------------------------------------------------------\n",
    "    def train(self, show=False, iters=10000000):\n",
    "        self.W_x_to_hidden = np.random.randn(1, self.vocab_size)\n",
    "        self.W_hidden_to_hidden = np.array([[0.]])\n",
    "        self.W_hidden_to_y = np.random.randn(self.vocab_size, 1)\n",
    "    \n",
    "        self.bias_hidden = np.random.randn(1, 1)\n",
    "        self.bias_output = np.random.randn(self.vocab_size, 1)\n",
    "\n",
    "        text = list(self.text)\n",
    "        inputs = text[:len(text) - 1]\n",
    "        targets = text[1:]\n",
    "        \n",
    "        learning_rate = 0.1\n",
    "        \n",
    "        mWxh, mWhh, mWhy =  np.zeros_like(self.W_x_to_hidden), \\\n",
    "                            np.zeros_like(self.W_hidden_to_hidden), \\\n",
    "                            np.zeros_like(self.W_hidden_to_y)\n",
    "        mbh, mby = np.zeros_like(self.bias_hidden), np.zeros_like(self.bias_output) # memory variables for Adagrad\n",
    "        \n",
    "        last_n = 0\n",
    "        prev_loss = None\n",
    "        h_prev = self.h.copy()\n",
    "        \n",
    "        for n in range(iters):\n",
    "            last_n = n\n",
    "            loss, dWxh, dWhh, dWhy, dbh, dby, h_prev = self.loss(inputs, targets, h_prev)\n",
    "            \n",
    "            if n % 10000 == 0: \n",
    "                print('iter {}, loss: {}'.format(n, loss))\n",
    "                if prev_loss is not None and np.abs(prev_loss - loss) < 0.01:\n",
    "                    break\n",
    "                prev_loss = loss\n",
    "            \n",
    "            for param, dparam, mem in zip(\n",
    "                    [self.W_x_to_hidden, \n",
    "                        self.W_hidden_to_hidden, \n",
    "                        self.W_hidden_to_y, \n",
    "                        self.bias_hidden, \n",
    "                        self.bias_output],\n",
    "                    [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                    [mWxh, mWhh, mWhy, mbh, mby]\n",
    "            ):\n",
    "                mem += dparam ** 2\n",
    "                param += -learning_rate * dparam / np.sqrt(mem + 1e-8)  # adagrad\n",
    "                \n",
    "        print('iter {}, loss: {}'.format(last_n, loss))\n",
    "        \n",
    "        if show:\n",
    "            self.print_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 41.49437051279101\n",
      "iter 10000, loss: 14.988488146942387\n",
      "iter 20000, loss: 14.290996060902039\n",
      "iter 29999, loss: 11.17479173022433\n",
      "('i', array([[ 0.]]))\n",
      "(' ', -0.9999882602923269)\n",
      "(' ', -0.98406058506244398)\n",
      "('w', 0.4771155482050799)\n",
      "(' ', -0.99999647163065986)\n",
      "('s', 0.99995421584185984)\n",
      "(' ', -0.86167016020888954)\n",
      "('t', -0.19006165864219066)\n",
      "(' ', -0.99999869546271736)\n",
      "('t', -0.49068048332567965)\n",
      "(' ', -0.92270134247109592)\n",
      "('b', 0.16105263348730725)\n",
      "(' ', -0.95883912182149478)\n",
      "('s', 0.80590078258747466)\n",
      "('t', -0.169801751476059)\n"
     ]
    }
   ],
   "source": [
    "n = RNN('it was the best')\n",
    "n.train(iters=300000)\n",
    "\n",
    "\n",
    "n.reset()\n",
    "print((n.text[0], n.h))\n",
    "\n",
    "\n",
    "for c in list(n.text[:-1]):\n",
    "    next = n.step(c)\n",
    "    print(next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
